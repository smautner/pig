{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA\n",
    "\n",
    "\n",
    "NOTE: SS_cons symbols can be looked up in the infernal dokumentation\n",
    "\n",
    "\n",
    "\n",
    "definition block:  bunch of adjacent < or > in the structure, now adding all other stems and a counter \n",
    "\n",
    "len + lennogap ,count   ,, length of alignments sometimes removing gaps, number of sequences in the alignment\n",
    "\n",
    "aliX , ali1, ali2 , removing badly aligned sequences: 20%, 1 ,2   \n",
    "calculation: rank sequences by outlier-points: if a column has 1 '.', or all but 1 '.' count an outlier-point\n",
    "\n",
    "filtered: removing blocks of ~size~ len <3\n",
    "\n",
    "flank: selected the positions that are within 5nuc of a block\n",
    "\n",
    "cons, cov:  \n",
    "calculation: mean([(max(Y,R)> .5 and (number of dots) else 0) / count])  \n",
    "calculation: mean([(max(character) / count] and . becomes 0)  \n",
    "calculation: cov/len \n",
    "\n",
    "stem length last-1: size of the second largest block \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import loadfiles\n",
    "p,n  = loadfiles.loaddata(\"/home/pig/data\",numneg=2000 if not debug else 200, pos='1' if debug else 'both', seed = 9)\n",
    "#X1,X2,Y1,Y2 = train_test_split(test_size=.33, random_state=1337)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(loadfiles.fnames_to_dict(['/home/pig/data/neg/769-969-0-0.sto']))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "allfeatures = list(p[1].keys()) # the filenames are the last one and we dont need that (for now)\n",
    "allfeatures.remove(\"name\")\n",
    "pprint.pprint(list(p[1].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a pandas dataframe, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "def clean(di,oklist):\n",
    "    for k in list(di.keys()):\n",
    "        if k not in oklist:\n",
    "            di.pop(k)\n",
    "    return di\n",
    "\n",
    "def makeXY(featurelist):\n",
    "    asd = [ clean(e,featurelist) for e in copy.deepcopy(p+n) ]\n",
    "    df = pd.DataFrame(asd)\n",
    "    X= df.to_numpy()\n",
    "    y= [1]*len(p)+[0]*len(n)\n",
    "    return X,y,df\n",
    "\n",
    "\n",
    "X,y,df = makeXY(allfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bonus, hacked in last minute before progress report :) \n",
    "import seaborn as sns\n",
    "def draw_ft(name):\n",
    "    sns.distplot(df[name][:len(p)], color='r', label='pos')\n",
    "    sns.distplot(df[name][len(p):], color='b', label='neg')\n",
    "    plt.legend();\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if is nan\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "#display(HTML(df.to_html()))\n",
    "display(HTML(df[df.isna().any(axis=1)].head().to_html()))\n",
    "\n",
    "#print(df[\" number of { blocks\"])\n",
    "\n",
    "# draw all distributions to to see if there is anything suspicious\n",
    "#for e in allfeatures:\n",
    "#    print(e)\n",
    "#    draw_ft(e)\n",
    "\n",
    "#df[df.isna().any(axis=1)].to_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV as rec\n",
    "X = StandardScaler().fit_transform(X)\n",
    "from sklearn.linear_model import Lasso\n",
    "randseed = 42\n",
    "testsize=.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testsize, random_state=randseed) # USE THE SAME SEED AS BELOW! \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def myscore(y,yh):\n",
    "    tpr = sum([ i==j for i,j in zip(y,yh) if i==1  ])/np.count_nonzero(y==1)\n",
    "    tnr = sum([ i==j for i,j in zip(y,yh) if i==0  ])/np.count_nonzero(y==0)\n",
    "    return ((2*tnr)+tpr)/3\n",
    "    \n",
    "def scorer(esti,X,y):\n",
    "    yh = esti.predict(X)\n",
    "    return myscore(y,yh)\n",
    "\n",
    "if False:\n",
    "    algo=[\n",
    "        #RandomForestClassifier(max_depth = 9, n_estimators=100 if not debug else 50, class_weight='balanced') # 100 to make it stable\n",
    "        #RandomForestClassifier( n_estimators=25)\n",
    "        #SVC(kernel=\"linear\",class_weight='balanced', C=0.025)\n",
    "        #DecisionTreeClassifier(max_depth=10,class_weight='balanced',min_samples_leaf=4)\n",
    "        #DecisionTreeClassifier()\n",
    "    ]\n",
    "    for e in algo:\n",
    "        sel=  rec(e,n_jobs = -1 , scoring = scorer, min_features_to_select=10)\n",
    "        #sel=  rec(e,n_jobs = -1 , min_features_to_select=10)\n",
    "        sel = sel.fit(X_train, y_train)\n",
    "        pprint.pprint([b for a,b in zip(sel.support_, df.columns) if a])\n",
    "        FEATURELIST = [b for a,b in zip(sel.support_, df.columns) if a]\n",
    "        \n",
    "mod = Lasso(alpha=.05)\n",
    "mod.fit(X_train,y_train)\n",
    "print('asd',mod.sparse_coef_)\n",
    "print('asd2',mod.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,df = makeXY(FEATURELIST)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testsize, random_state=randseed) # USE THE SAME SEED AS BELOW! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ft importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ft_importance(forest,ftnames):\n",
    "    lenft=len(ftnames)\n",
    "    importances = forest.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "    print (indices,lenft)\n",
    "    ranked_features = [ftnames[indices[f]] for f in range(lenft)]\n",
    "    pprint.pprint(ranked_features)\n",
    "    \n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(lenft), importances[indices],\n",
    "           color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(lenft), indices)\n",
    "    plt.xlim([-1, lenft])\n",
    "    plt.show()\n",
    "    return ranked_features\n",
    "\n",
    "fo = RandomForestClassifier(max_depth = 9, n_estimators=20, class_weight='balanced') \n",
    "fo.fit(X_train, y_train)\n",
    "topfeatures = ft_importance(fo,FEATURELIST)[:6 if debug else 15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check performance for various classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "names = [\"Nearest Neighbors\",\"Linear SVM\", \"RBF SVM\",\n",
    "         #\"Gaussian Process\", # 2 slow\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(5),\n",
    "    SVC(kernel=\"linear\",class_weight='balanced', C=0.025),\n",
    "    SVC(gamma=2, C=1,class_weight='balanced'),\n",
    "    #SVC(),\n",
    "    #GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5,class_weight='balanced',min_samples_leaf=4),\n",
    "    RandomForestClassifier(max_depth=9, n_estimators=30, class_weight='balanced'),\n",
    "    #RandomForestClassifier(max_depth=5, n_estimators=25, max_features=5,class_weight='balanced'),\n",
    "    MLPClassifier(alpha=.001, max_iter=2000),\n",
    "    AdaBoostClassifier(DecisionTreeClassifier(max_depth=3)),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()\n",
    "    ]\n",
    "\n",
    "#X = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testsize, random_state=randseed)\n",
    "\n",
    "import draw\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    print (\"%s acc:%f2 myscore:%f2\"% (name, score, scorer(clf,X_test,np.array(y_test))))\n",
    "    #print (clf.predict(X_test))\n",
    "    draw.matrix(y_test,clf.predict(X_test), np.array(['bad','good']),normalize=False,title =name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inspect decission tree \n",
    "\n",
    "i think this is interesting because it shows how one would generate a manual decider, and when considering the leafes\n",
    "one can see how efficient rules look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import tree\n",
    "from sklearn.datasets import load_wine\n",
    "from IPython.display import SVG, Image\n",
    "from graphviz import Source\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "estimator = DecisionTreeClassifier(max_depth=5,class_weight='balanced',min_samples_leaf=4)\n",
    "estimator.fit(X,y)  # just fit on all data for demonstation :) \n",
    "graph = Source(tree.export_graphviz(estimator, out_file=None\n",
    "   , feature_names=df.columns, class_names=['0', '1'] \n",
    "   , filled = True))\n",
    "\n",
    "\n",
    "display(Image(graph.pipe(format='png')))\n",
    "#display(SVG(graph.pipe(format='svg')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature pairmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "X,y,df = makeXY(topfeatures)\n",
    "\n",
    "df.insert(len(topfeatures),'class',[str(x)+\" class\" for x in y ])\n",
    "if True:\n",
    "    sns.pairplot(df, hue=\"class\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pig/.config/scripts/python:\n"
     ]
    }
   ],
   "source": [
    "!echo $PYTHONPATH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/pig/.local/lib/python3.6/site-packages/matplotlib/__init__.py'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
